import json
from transformers import AutoTokenizer, LlamaForCausalLM
from fastchat.model.model_adapter import get_conversation_template
import torch
from tqdm import tqdm
import random
import argparse

@torch.inference_mode()
def get_jacobian_trajectory(
    model,
    tokenizer,
    input_ids,
    attention_mask,
    max_new_tokens
):

    bsz = input_ids.shape[0] 
    prompt_len = [torch.sum(t) for t in attention_mask]
    max_prompt_len = max(prompt_len)
    total_len = max_prompt_len + max_new_tokens

    # initialize the first point of jacobian trajectory
    tokens = torch.full((bsz, total_len), tokenizer.pad_token_id, dtype=torch.long, device="cuda")
    for i in range(bsz):
        tokens[i, :] = torch.tensor(random.choices(input_ids[i][attention_mask[i]==1], k=total_len), dtype=torch.long, device="cuda")
        tokens[i, : prompt_len[i]] = torch.tensor(input_ids[i][: prompt_len[i]], dtype=torch.long, device="cuda")
    trajectory = []
    logits_trajectory = []
    next_generation = tokens
    generate_attention_mask = torch.full_like(next_generation, 1).to(tokens.device)
    trajectory.append(tokens)
    itr=0
    while True:
        
        current_generation = next_generation
        logits = model(current_generation, generate_attention_mask).logits
        logits_trajectory.append(logits)
        next_generation = torch.argmax(torch.nn.functional.softmax(logits, dim=-1), dim=-1)

        # hold prompt unchanged and update generated tokens
        for i in range(bsz):
            next_generation[i, :] = torch.cat((tokens[i, :prompt_len[i]], next_generation[i, prompt_len[i]-1:total_len-1]), dim=0)
        trajectory.append(next_generation)
        if torch.all(torch.eq(next_generation, current_generation)).item():
            eos_reached = len(torch.where(trajectory[-1] == tokenizer.eos_token_id)[0])>0
            print(f'Iteration steps: {itr}')
            return trajectory[:-1], logits_trajectory[-2], eos_reached # right generation is saved twice so we delete the last element of trajectory list
        itr+=1

def main(filename, model, tokenizer, max_new_tokens, max_new_seq_len, use_aug):

    if 'wizard' in filename.lower():

        with open(filename) as f:
            data = json.load(f)

        new_data = []
        # only support batch size ==1 now
        for d in tqdm(data[:500]):
            if len(d["conversations"]) > 2:
                continue
            prompt = d["conversations"][0]["value"]
            conv = get_conversation_template(model_path)
            conv.append_message(conv.roles[0], prompt)
            conv.append_message(conv.roles[1], "")
            prompt_with_template = conv.get_prompt()
            jacobian_prompt = prompt_with_template
            if tokenizer(jacobian_prompt, return_tensors="pt").input_ids.shape[1] > 300:
                continue
            itr = 0
            eos_reached=False
            # while itr * max_new_tokens < max_new_seq_len and eos_reached==False:
            while eos_reached==False:
                dic = {}
                dic['idx']=f'{d["idx"]}_{itr}'
                dic['jacobian_prompt'] = jacobian_prompt
                inputs = tokenizer(jacobian_prompt, return_tensors="pt").to(model.device)
                jacobian_trajectory_ids, teacher_logits, eos_reached = get_jacobian_trajectory(model, tokenizer, inputs['input_ids'], inputs['attention_mask'], max_new_tokens)
                dic["answer_trajectory_ids"] = []
                for i, id in enumerate(jacobian_trajectory_ids): 
                    # only support batch size=1 now
                    dic["answer_trajectory_ids"].append(id[0][-max_new_tokens:].tolist()) 

                if use_aug:
                    for j in range(len(dic["answer_trajectory_ids"])-3, 0, -1):
                        correct_positions = torch.where(torch.tensor(dic["answer_trajectory_ids"][j]!=dic["answer_trajectory_ids"][-1]))[0]
                        for correct_id in random.choices(correct_positions, k=8):
                            aug_trajectory = dic["answer_trajectory_ids"][j].copy()
                            aug_trajectory[correct_id] = dic["answer_trajectory_ids"][-1][correct_id]
                        dic["answer_trajectory_ids"].insert(0, aug_trajectory)

                jacobian_prompt = tokenizer.decode(jacobian_trajectory_ids[-1][0])[4:] # 4: is to remove the <s>(bos) generated by tokenizer
                print(jacobian_prompt)
                new_data.append(dic)
                itr+=1
                
            with open(f"{filename.split('.')[0]}_jacobian{max_new_tokens}_aug{use_aug}_no_max_seq_len.json", "w") as f:
                json.dump(new_data, f)
    else:

        with open(filename) as f:
            data = json.load(f)
        # print(len(data))
            
        new_data = []
        # only support batch size ==1 now
        for d in tqdm(data[:2000]):
            prompt = d["conversation"][0]["content"]
            conv = get_conversation_template(model_path)
            conv.append_message(conv.roles[0], prompt)
            conv.append_message(conv.roles[1], "")
            prompt_with_template = conv.get_prompt()
            jacobian_prompt = prompt_with_template
            itr = 0
            eos_reached=False
            while itr * max_new_tokens < max_new_seq_len and eos_reached==False:
            # while eos_reached==False:
                dic = {}
                dic['id']=f'{d["id"]}_{itr}'
                dic['jacobian_prompt'] = jacobian_prompt
                inputs = tokenizer(jacobian_prompt, return_tensors="pt").to(model.device)
                jacobian_trajectory_ids, teacher_logits, eos_reached = get_jacobian_trajectory(model, tokenizer, inputs['input_ids'], inputs['attention_mask'], max_new_tokens)
                dic["answer_trajectory_ids"] = []
                for i, id in enumerate(jacobian_trajectory_ids): 
                    # only support batch size=1 now
                    dic["answer_trajectory_ids"].append(id[0][-max_new_tokens:].tolist()) 

                if use_aug:
                    for j in range(len(dic["answer_trajectory_ids"])-3, 0, -1):
                        correct_positions = torch.where(torch.tensor(dic["answer_trajectory_ids"][j]!=dic["answer_trajectory_ids"][-1]))[0]
                        for correct_id in random.choices(correct_positions, k=8):
                            aug_trajectory = dic["answer_trajectory_ids"][j].copy()
                            aug_trajectory[correct_id] = dic["answer_trajectory_ids"][-1][correct_id]
                        dic["answer_trajectory_ids"].insert(0, aug_trajectory)

                jacobian_prompt = tokenizer.decode(jacobian_trajectory_ids[-1][0])[4:] # 4: is to remove the <s>(bos) generated by tokenizer
                print(jacobian_prompt)
                new_data.append(dic)
                itr+=1
                
            with open(f"{filename.split('.')[0]}_jacobian{max_new_tokens}_aug{use_aug}_max_seq_len_128.json", "w") as f:
                json.dump(new_data, f)


def eval(filename, model, tokenizer, max_new_tokens, max_new_seq_len):

    with open(filename) as f:
        data = json.load(f)

    # only support batch size ==1 now
    for d in tqdm(data[1011:1012]):
        if len(d["conversations"]) > 2:
            continue
        prompt = d["conversations"][0]["value"]
        conv = get_conversation_template(model_path)
        conv.append_message(conv.roles[0], prompt)
        conv.append_message(conv.roles[1], "")
        prompt_with_template = conv.get_prompt()
        jacobian_prompt = prompt_with_template
        if tokenizer(jacobian_prompt, return_tensors="pt").input_ids.shape[1] > 300:
            continue
        itr = 0
        eos_reached=False
        while itr * max_new_tokens < max_new_seq_len and eos_reached==False:
            inputs = tokenizer(jacobian_prompt, return_tensors="pt").to(model.device)
            jacobian_trajectory_ids, teacher_logits, eos_reached = get_jacobian_trajectory(model, tokenizer, inputs['input_ids'], inputs['attention_mask'], max_new_tokens) 
            jacobian_prompt = tokenizer.decode(jacobian_trajectory_ids[-1][0])[4:] # 4: is to remove the <s>(bos) generated by tokenizer
            print(f'Total length: {jacobian_trajectory_ids[-1].shape}')
            print(jacobian_prompt)
            itr+=1

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--filename", type=str,
                        default="/liymai24/sjtu/siqi/wizard_dataset/WizardLM_evol_instruct_V2_143k.json")
    parser.add_argument("--max_new_tokens", type=int, default=16)
    parser.add_argument("--max_new_seq_len", type=int, default=128)
    parser.add_argument("--model", type=str,
                        default="/liymai24/sjtu/siqi/llm-model/vicuna-7b-v1.5")
    parser.add_argument("--use_aug", default=False)
    args = parser.parse_args()
    filename = args.filename
    model_path = args.model
    max_new_tokens = args.max_new_tokens
    max_new_seq_len = args.max_new_seq_len
    model = LlamaForCausalLM.from_pretrained(model_path, device_map='auto', 
                                             torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="right", use_fast=False)
    main(filename, model, tokenizer, max_new_tokens, max_new_seq_len, args.use_aug)
    # eval(filename, model, tokenizer, max_new_tokens, max_new_seq_len) # CUDA_VISIBLE_DEVICES=0 python generate_trajectory_data.py --model /liymai24/sjtu/siqi/llm-model/vicuna-7b_wizard_16/global/checkpoint-1200
