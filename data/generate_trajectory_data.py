import json
from transformers import AutoTokenizer, LlamaForCausalLM
from fastchat.model.model_adapter import get_conversation_template
import torch
from tqdm import tqdm
import random
import argparse

@torch.inference_mode()
def get_jacobian_trajectory(
    model,
    tokenizer,
    input_ids,
    attention_mask,
    max_new_tokens
):

    bsz = input_ids.shape[0] 
    prompt_len = [torch.sum(t) for t in attention_mask]
    max_prompt_len = max(prompt_len)
    total_len = max_prompt_len + max_new_tokens

    # initialize the first point of jacobian trajectory
    tokens = torch.full((bsz, total_len), tokenizer.pad_token_id, dtype=torch.long, device="cuda")
    for i in range(bsz):
        tokens[i, :] = torch.tensor(random.choices(input_ids[i][attention_mask[i]==1], k=total_len), dtype=torch.long, device="cuda")
        tokens[i, : prompt_len[i]] = torch.tensor(input_ids[i][: prompt_len[i]], dtype=torch.long, device="cuda")
    trajectory = []
    logits_trajectory = []
    next_generation = tokens
    generate_attention_mask = torch.full_like(next_generation, 1).to(tokens.device)
    trajectory.append(tokens)
    itr=0
    while True:
        
        current_generation = next_generation
        logits = model(current_generation, generate_attention_mask).logits
        logits_trajectory.append(logits)
        next_generation = torch.argmax(torch.nn.functional.softmax(logits, dim=-1), dim=-1)

        # hold prompt unchanged and update generated tokens
        for i in range(bsz):
            next_generation[i, :] = torch.cat((tokens[i, :prompt_len[i]], next_generation[i, prompt_len[i]-1:total_len-1]), dim=0)
        trajectory.append(next_generation)
        if torch.all(torch.eq(next_generation, current_generation)).item():
            eos_reached = len(torch.where(trajectory[-1] == tokenizer.eos_token_id)[0])>0
            return trajectory[:-1], logits_trajectory[-2], eos_reached # right generation is saved twice so we delete the last element of trajectory list
        itr+=1

def main(filename, model, tokenizer, max_new_tokens, max_new_seq_len):

    with open(filename) as f:
        data = json.load(f)

    new_data = []
    # only support batch size ==1 now
    for d in tqdm(data[:1000]):
        if len(d["conversations"]) > 2:
            continue
        prompt = d["conversations"][0]["value"]
        conv = get_conversation_template(model_path)
        conv.append_message(conv.roles[0], prompt)
        conv.append_message(conv.roles[1], "")
        prompt_with_template = conv.get_prompt()
        jacobian_prompt = prompt_with_template
        itr = 0
        eos_reached=False
        while itr * max_new_tokens < max_new_seq_len and eos_reached==False:
            dic = {}
            dic['idx']=f'{d["idx"]}_{itr}'
            dic['jacobian_prompt'] = jacobian_prompt
            inputs = tokenizer(jacobian_prompt, return_tensors="pt").to(model.device)
            # print(inputs['input_ids'].shape)
            jacobian_trajectory_ids, teacher_logits, eos_reached = get_jacobian_trajectory(model, tokenizer, inputs['input_ids'], inputs['attention_mask'], max_new_tokens)
            # print(jacobian_trajectory_ids[0].shape)
            # dic["teacher_logits"] = teacher_logits.tolist()
            dic["answer_trajectory_ids"] = []
            for i, id in enumerate(jacobian_trajectory_ids): 
                # only support batch size=1 now
                dic["answer_trajectory_ids"].append(id[0][-max_new_tokens:].tolist()) 
            jacobian_prompt = tokenizer.decode(jacobian_trajectory_ids[-1][0])[4:] # 4: is to remove the <s>(bos) generated by tokenizer
            print(jacobian_prompt)
            new_data.append(dic)
            itr+=1
            
        with open(f"{filename.split('.')[0]}_jacobian{max_new_tokens}.json", "w") as f:
            json.dump(new_data, f)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--filename", type=str,
                        default="/liymai24/sjtu/siqi/wizard_dataset/WizardLM_evol_instruct_V2_143k.json")
    parser.add_argument("--max_new_tokens", type=int, default=16)
    parser.add_argument("--max_new_seq_len", type=int, default=128)
    parser.add_argument("--model", type=str,
                        default="/liymai24/sjtu/siqi/llm-model/vicuna-7b-v1.5")
    args = parser.parse_args()
    filename = args.filename
    model_path = args.model
    max_new_tokens = args.max_new_tokens
    max_new_seq_len = args.max_new_seq_len
    model = LlamaForCausalLM.from_pretrained(model_path, device_map='auto', 
                                             torch_dtype=torch.bfloat16)
    tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side="right", use_fast=False)
    main(filename, model, tokenizer, max_new_tokens, max_new_seq_len)