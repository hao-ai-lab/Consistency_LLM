import transformers
import torch
from transformers import Trainer, TrainerCallback
from transformers.trainer_pt_utils import LabelSmoother
import wandb
from common import pad_to_2d, sychronize_time
from enum import Enum
import random
from torch.utils.data import DataLoader

import numpy as np

IGNORE_TOKEN_ID = LabelSmoother.ignore_index

import copy

class SampleSource(Enum):
    Student = 1
    Teacher = 2
    MixRequest = 3
    MixToken = 4


SAMPLE_SOURCE_MAP = {
    "student": SampleSource.Student,
    "teacher": SampleSource.Teacher,
    "mix_request": SampleSource.MixRequest,
    "mix_token": SampleSource.MixToken,
}


class KLMethod(Enum):
    Forward = 1
    Reverse = 2
    JSD = 3


KL_METHOD_MAP = {
    "forward": KLMethod.Forward,
    "reverse": KLMethod.Reverse,
    "jsd": KLMethod.JSD
}

eval_cnt = 0
# copy_model = transformers.AutoModelForCausalLM.from_pretrained(
#     "JackFram/llama-160m")
# copy_model.cuda()


class DistillTrainer(Trainer):
    def __init__(self, teacher_model, *args, **kwargs):
        super().__init__(*args, **kwargs)
        args = kwargs["args"]
        self.train_step_cnt = 0
        self.teacher_model = teacher_model

        # online related params
        self.mode = args.mode
        self.online_eval_interval = args.online_eval_interval
        self.online_update_interval = args.online_update_interval
        self.buffer = []
        self.sample_steps = []

        self.sample_source = SAMPLE_SOURCE_MAP[args.sample_source]
        self.kl_method = KL_METHOD_MAP[args.kl_method]

        self.max_new_tokens = args.max_new_tokens
        self.max_new_seq_len = args.max_new_seq_len
        self.consistency_loss = args.consistency_loss

    def training_step(self, model, inputs):
        self.train_step_cnt += 1
        return self.consistency_training_step(model, inputs)

    def consistency_training_step(self, model, inputs):

        max_new_tokens = self.max_new_tokens
        student_temperature = 1.0
        teacher_temperature = 1.0        

        jacobian_trajectory = inputs["jacobian_trajectory"]
        input_masks = inputs["attention_mask"]
        bsz = jacobian_trajectory[0].shape[0]
        eos_reached = torch.tensor([False] * bsz).to(jacobian_trajectory[0].device)
        print('Compute teacher logits:')
        with torch.no_grad():
            teacher_logits = self.get_logits(self.teacher_model, jacobian_trajectory[-1].to(self.teacher_model.device), input_masks.to(self.teacher_model.device))
        
        # print(self.tokenizer.decode(jacobian_trajectory[-1][0]))
        print('Teacher_logits computed!')

        ### tokens generated after <eos> are set to <pad>
        jacobian_trajectory_ = [trajectory.clone() for trajectory in jacobian_trajectory]
        for i in range(len(jacobian_trajectory)):
            for j in range(bsz):
                trajectory_len = torch.sum(input_masks, dim=-1)
                # find the first accurate <EOS>
                eos_positions = torch.where(jacobian_trajectory_[i][j, :(trajectory_len[j]-max_new_tokens)]==self.tokenizer.eos_token_id)[0]
                if len(eos_positions)==0:
                    continue
                # otherwise, set tokens coming after the accurate <EOS> as pad 
                eos_reached[j] = True
                trajectory_copy = jacobian_trajectory[i].clone().detach()
                eos_pos = eos_positions[0]
                trajectory_copy[j, int(eos_pos)+1:] = self.tokenizer.pad_token_id
                jacobian_trajectory_[i] = trajectory_copy   
            
        ### compute AutoRegression loss ###
        attention_mask = torch.full_like(jacobian_trajectory[0], 1).to(jacobian_trajectory[0].device)
        attention_mask = jacobian_trajectory_[-1] != self.tokenizer.pad_token_id
        logits_last =  self.get_logits(model, jacobian_trajectory_[-1].clone().detach(), attention_mask)
        print('logits computed!')

        # ignore pad_token and prompt_token because we do not intend to calculate the cross entrophy loss w.r.t pad & prompt
        output_mask = jacobian_trajectory_[-1][..., 1:] == self.tokenizer.pad_token_id 

        print('computing loss...')
        loss_ar = self.soft_cross_entropy(
            logits_last[..., :-1, :].float() / student_temperature, # logits generated by the last token is dropped
            teacher_logits.to(logits_last.device)[..., :-1, :].float() / teacher_temperature,
            output_mask.to(logits_last.device)
        )
        print(f'loss ar: {loss_ar} computed! performing backward pass...')
        with self.accelerator.accumulate(model):
            self.accelerator.backward(loss_ar)
        print('backward pass done!')

        ### compute Consistency global loss ###
        # random select one point from trajectory (all right tokens point is excepted)
        i = random.choice(range(len(jacobian_trajectory_))[:-1])

        attention_mask = torch.full_like(jacobian_trajectory[0], 1).to(jacobian_trajectory[0].device)
        attention_mask = jacobian_trajectory_[i] != self.tokenizer.pad_token_id
        logits_i = self.get_logits(model, jacobian_trajectory_[i].clone().detach(), attention_mask)

        output_mask = jacobian_trajectory_[i][..., 1:] == self.tokenizer.pad_token_id
        # We do not calculate the cross entrophy of same logits to alleviate misleading gradients
        for j in range(bsz):
            end_of_mask_position = torch.where(jacobian_trajectory_[i][j, 1:] != jacobian_trajectory_[-1][j, 1:])[0]
            if len(end_of_mask_position)==0:
                output_mask[j, :] = True
            else:
                output_mask[j, :end_of_mask_position[0]] = True
        
        print('computing loss...')
        loss_global = self.soft_cross_entropy(
                    logits_i[..., :-1, :].float() / student_temperature, # logits generated by the last token is dropped
                    logits_last[..., :-1, :].to(logits_i.device).clone().detach().float() / student_temperature,
                    output_mask.to(logits_i.device)
        )

        print(f'loss global {loss_global} computed! performing backward pass...')
        with self.accelerator.accumulate(model):
            self.accelerator.backward(loss_global)
        print('backward pass done!')

        # sync processes
        torch.distributed.barrier()
        # total loss = ar_loss + consistency_global_loss
        loss = loss_ar.detach()+loss_global.detach()

        return loss
    

    def log(self, logs):
        # Remove the 'loss' entry with value 0 before calling the superclass method
        if 'loss' in logs and logs['loss'] == -1:
            del logs['loss']

        # Call the original `log` method of the `Trainer` class
        super().log(logs)

    def get_train_dataloader(self):
        # Create custom DataLoader with shuffle set to False
        shuffle = False if self.mode == "online" else True
        dataloader_params = {
            "batch_size": self.args.per_device_train_batch_size,
            "shuffle": shuffle,
            "num_workers": self.args.dataloader_num_workers,
            "pin_memory": self.args.dataloader_pin_memory,
        }

        return self.accelerator.prepare(DataLoader(self.train_dataset, **dataloader_params))

    ###################### Helper Functions #############################
    def soft_cross_entropy(self, predicts, targets, padding_mask):
        # TODO: support batch_size >1 here.
        if (~padding_mask).sum() == 0:
            return 0*predicts[0][0][0]
        predict_log_prob = torch.nn.functional.log_softmax(predicts, dim=-1)
        targets_prob = torch.nn.functional.softmax(targets, dim=-1)
        entropy = -targets_prob * predict_log_prob
        expand_mask = padding_mask.unsqueeze(-1).expand_as(entropy)
        entropy.masked_fill_(expand_mask, 0)
        mean_entropy = entropy.sum() / (~padding_mask).sum()
        return mean_entropy

    def get_kl(self, predicts, targets, padding_mask):
        kl_loss = torch.nn.KLDivLoss(reduction="none", log_target=True)
        predict_prob = torch.nn.functional.log_softmax(predicts, dim=-1)
        targets_prob = torch.nn.functional.log_softmax(targets, dim=-1)
        output = kl_loss(predict_prob, targets_prob)
        expand_mask = padding_mask.unsqueeze(-1).expand_as(output)
        output.masked_fill_(expand_mask, 0)
        mean_output = output.sum() / (~padding_mask).sum()
        return mean_output

    def get_logits(self, model, input_ids, attention_mask):
        return model(
            input_ids=input_ids,
            attention_mask=attention_mask,
        ).logits


class DistillTrainerCallback(TrainerCallback):
    def __init__(self) -> None:
        super().__init__()
        self.correct_cnt = 0
        self.propose_cnt = 0

        self.alpha = 0
        self.sample_steps = 0

    def on_evaluate(self, args, state, control, **kwargs):
        pass
